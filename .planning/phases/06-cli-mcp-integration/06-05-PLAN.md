---
phase: 06-cli-mcp-integration
plan: 05
type: execute
wave: 3
depends_on: ["06-02", "06-03", "06-04"]
files_modified:
  - tests/cli/cli.test.ts
  - tests/mcp/mcp.test.ts
autonomous: true

must_haves:
  truths:
    - "CLI subcommand routing dispatches correctly to all 6 commands"
    - "Config loader merges uidna.config.json with CLI flags correctly"
    - "MCP server registers expected resources and tools"
    - "All commands show helpful error messages when preconditions not met"
  artifacts:
    - path: "tests/cli/cli.test.ts"
      provides: "Unit tests for CLI config loader, progress utility, and command routing"
      contains: "describe.*cli"
    - path: "tests/mcp/mcp.test.ts"
      provides: "Unit tests for MCP server resource and tool registration"
      contains: "describe.*mcp"
  key_links:
    - from: "tests/cli/cli.test.ts"
      to: "src/cli/config-loader.ts"
      via: "imports and tests loadFullConfig"
      pattern: "import.*loadFullConfig"
    - from: "tests/mcp/mcp.test.ts"
      to: "src/mcp/server.ts"
      via: "imports and tests createMcpServer"
      pattern: "import.*createMcpServer"
---

<objective>
Write unit tests for CLI commands and MCP server to verify correctness and catch regressions.

Purpose: Ensure CLI and MCP integration layer works correctly, handles edge cases, and provides good error messages.
Output: Comprehensive test suites for CLI and MCP modules
</objective>

<execution_context>
@C:/Users/Karl/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Karl/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-cli-mcp-integration/06-01-SUMMARY.md
@.planning/phases/06-cli-mcp-integration/06-02-SUMMARY.md
@.planning/phases/06-cli-mcp-integration/06-03-SUMMARY.md
@.planning/phases/06-cli-mcp-integration/06-04-SUMMARY.md
@src/cli/config-loader.ts
@src/cli/progress.ts
@src/mcp/server.ts
@src/mcp/resources.ts
@src/mcp/tools.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Write CLI unit tests</name>
  <files>tests/cli/cli.test.ts</files>
  <action>
Create `tests/cli/cli.test.ts` using vitest:

**Config loader tests (describe "config-loader"):**
1. Test: loadFullConfig returns defaults when no config file exists
   - Call loadFullConfig with seedUrls only, verify defaults applied (maxDepth: 3, maxPages: 100, etc.)
2. Test: loadFullConfig merges config file values
   - Write temp uidna.config.json with `{ "maxDepth": 5, "maxPages": 200 }`
   - Call loadFullConfig, verify maxDepth=5, maxPages=200
   - Clean up temp file in afterEach
3. Test: CLI args override config file
   - Write temp config with maxPages: 200
   - Call loadFullConfig with { maxPages: 50, seedUrls: ["https://test.com"] }
   - Verify maxPages=50 (CLI wins)
4. Test: Invalid config file throws descriptive error
   - Write temp config with invalid JSON
   - Verify loadFullConfig throws with message about parse error
5. Test: Config validation catches invalid values
   - Write temp config with `{ "maxDepth": 999 }` (exceeds max 10)
   - Verify loadFullConfig throws with zod validation error

**Progress utility tests (describe "progress"):**
6. Test: withProgress resolves with task result
   - Call withProgress with a task that returns "result"
   - Verify return value is "result"
7. Test: withProgress propagates errors
   - Call withProgress with a task that throws
   - Verify error is rethrown

**For config tests that write files:** Use vitest's `beforeEach`/`afterEach` to create and clean up a temp directory. Write uidna.config.json in the temp dir and use `process.chdir()` (save original cwd in beforeEach, restore in afterEach) OR pass the config path explicitly to loadFullConfig if it supports a path parameter.

All tests should use fixture data only -- no network calls, no browser, no API keys. Follow project test convention: tests/ directory at project root.
  </action>
  <verify>Run `npx vitest run tests/cli/cli.test.ts` and confirm all tests pass.</verify>
  <done>CLI tests cover config loading with defaults, file merging, CLI overrides, validation errors, and progress utility behavior.</done>
</task>

<task type="auto">
  <name>Task 2: Write MCP server unit tests</name>
  <files>tests/mcp/mcp.test.ts</files>
  <action>
Create `tests/mcp/mcp.test.ts` using vitest:

**MCP server tests (describe "mcp-server"):**

1. Test: createMcpServer returns a server instance
   - Call createMcpServer with a temp data dir
   - Verify server object is returned (truthy)

2. Test: Resources are registered
   - Create server, use reflection or SDK inspection to verify resource names include: "design-tokens", "components", "patterns", "content-style", "brand-report"

3. Test: Tools are registered
   - Create server, verify tool names include: "synthesize_component", "export_format", "get_token"

**Resource handler tests (describe "mcp-resources"):**

4. Test: design-tokens resource returns token data when file exists
   - Create temp data dir with tokens.json fixture (minimal valid JSON)
   - Register resources, call handler for "design-tokens"
   - Verify response contains token data

5. Test: Resource returns error message when data file missing
   - Use empty temp data dir
   - Call handler for "design-tokens"
   - Verify response contains error about missing data

**Tool handler tests (describe "mcp-tools"):**

6. Test: get_token tool returns matching tokens
   - Create temp data dir with tokens.json fixture containing a few token entries
   - Call get_token handler with query "color"
   - Verify response contains matching token data

7. Test: export_format tool generates CSS output
   - Create temp data dir with tokens.json fixture
   - Call export_format with format "css"
   - Verify response contains CSS custom property format

NOTE: Tests should NOT require a running MCP server or stdio transport. Test the registration functions and handlers directly by importing them. For handler testing, create the MCP server with createMcpServer and test resource/tool handlers via the SDK's internal APIs, OR test the handler functions in isolation if they're separated enough.

If the SDK does not expose direct handler testing, test the handler logic by extracting it into testable helper functions within resources.ts and tools.ts, and test those helpers.

All tests use fixture data in temp directories -- no network calls, no API keys.
  </action>
  <verify>Run `npx vitest run tests/mcp/mcp.test.ts` and confirm all tests pass. Run `npx vitest run tests/cli/ tests/mcp/` to verify all Phase 6 tests pass together.</verify>
  <done>MCP tests cover server creation, resource registration, tool registration, resource data retrieval, error handling for missing data, and tool execution.</done>
</task>

</tasks>

<verification>
- `npx vitest run tests/cli/cli.test.ts` -- all tests pass
- `npx vitest run tests/mcp/mcp.test.ts` -- all tests pass
- `npx vitest run` -- all project tests pass (no regressions)
- No tests require network, browser, or API keys
</verification>

<success_criteria>
- CLI config loader tested with 5+ scenarios (defaults, merge, override, invalid JSON, validation)
- Progress utility tested for success and error paths
- MCP server tested for resource/tool registration and handler behavior
- All tests pass without external dependencies (no network, no API keys)
</success_criteria>

<output>
After completion, create `.planning/phases/06-cli-mcp-integration/06-05-SUMMARY.md`
</output>
