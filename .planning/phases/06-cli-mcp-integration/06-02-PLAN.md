---
phase: 06-cli-mcp-integration
plan: 02
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - src/cli/commands/crawl.ts
  - src/cli/commands/extract.ts
autonomous: true

must_haves:
  truths:
    - "User can run `uidna crawl <url>` with --max-depth, --max-pages, and --domain options"
    - "User can run `uidna extract` to normalize tokens and detect components/patterns on crawled data"
    - "Both commands show progress spinners and clear completion summaries"
  artifacts:
    - path: "src/cli/commands/crawl.ts"
      provides: "Crawl command handler with URL, depth, page limit, and domain allowlist options"
      exports: ["crawlCommand"]
    - path: "src/cli/commands/extract.ts"
      provides: "Extract command handler that runs normalization, component mining, and pattern detection"
      exports: ["extractCommand"]
  key_links:
    - from: "src/cli/commands/crawl.ts"
      to: "src/orchestrator.ts"
      via: "calls runPipeline with config from loadFullConfig"
      pattern: "import.*runPipeline.*from.*orchestrator"
    - from: "src/cli/commands/extract.ts"
      to: "src/normalization/normalize-pipeline.ts"
      via: "calls normalization pipeline on stored token data"
      pattern: "import.*normali"
    - from: "src/cli.ts"
      to: "src/cli/commands/crawl.ts"
      via: "dynamic import in switch case"
      pattern: "import.*cli/commands/crawl"
---

<objective>
Implement the crawl and extract CLI commands -- the two data-gathering commands that produce raw tokens and normalized design DNA.

Purpose: These are the primary entry points for users. Crawl fetches data from a website, extract transforms it into design tokens and components.
Output: Working crawl and extract commands with progress feedback
</objective>

<execution_context>
@C:/Users/Karl/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Karl/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-cli-mcp-integration/06-RESEARCH.md
@.planning/phases/06-cli-mcp-integration/06-01-SUMMARY.md
@src/orchestrator.ts
@src/shared/config.ts
@src/normalization/normalize-pipeline.ts
@src/components/component-aggregator.ts
@src/content/index.ts
@src/patterns/index.ts
@src/storage/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement crawl command</name>
  <files>src/cli/commands/crawl.ts, src/cli.ts</files>
  <action>
Create `src/cli/commands/crawl.ts`:

Export `crawlCommand(args: string[])` async function:

1. Parse command-specific flags from args array:
   - First non-flag arg = seed URL (required, error if missing)
   - `--max-depth N` (number, optional)
   - `--max-pages N` (number, optional)
   - `--domain <domain>` (string, repeatable -- each adds to domainAllowlist array)
   - `--output-dir <dir>` (string, optional)
   - `--concurrency N` (number, optional)
   - `--no-robots` (boolean flag)
   - `--help` (print crawl-specific help and exit)

2. Load config via `loadFullConfig({ seedUrls: [url], maxDepth, maxPages, domainAllowlist, outputDir, maxConcurrency, respectRobotsTxt })` -- only include non-undefined values.

3. Print crawl header: seed URL, max pages, max depth, output dir.

4. Use `withProgress` from `../progress.js`:
   - Initial message: "Crawling {url}..."
   - Update callback: `"Crawling: {currentUrl} ({pagesProcessed}/{maxPages} pages)"`
   - Call `runPipeline({ config, onProgress })` from `../../orchestrator.js`

5. After completion, print summary:
   - Pages crawled / failed / skipped (robots.txt)
   - Token counts per type (colors, typography, spacing, etc.)
   - Evidence entries count
   - Output directory path
   - If re-crawl: diff summary (added/removed/changed/unchanged)

6. On error: print user-friendly message ("Crawl failed: {message}"), exit(1)

7. Crawl-specific help:
   ```
   Usage: uidna crawl <url> [options]

   Crawl a website and extract design tokens.

   Options:
     --max-depth N       Maximum link depth (default: 3)
     --max-pages N       Maximum pages to crawl (default: 100)
     --domain <domain>   Allowed domain (repeatable)
     --output-dir <dir>  Output directory (default: .uidna)
     --concurrency N     Max concurrent requests (default: 5)
     --no-robots         Disable robots.txt checking
   ```

Update `src/cli.ts` to wire the `crawl` case:
- Replace placeholder with: `const { crawlCommand } = await import('./cli/commands/crawl.js'); await crawlCommand(process.argv.slice(3));`
  </action>
  <verify>Run `npx tsc --noEmit` to confirm no type errors. Run `npx tsx src/cli.ts crawl --help` to see crawl-specific help. Run `npx tsx src/cli.ts crawl` (no URL) to see error message about missing URL.</verify>
  <done>Users can run `uidna crawl <url>` with all options, see progress during crawl, and get a summary of extracted tokens.</done>
</task>

<task type="auto">
  <name>Task 2: Implement extract command</name>
  <files>src/cli/commands/extract.ts, src/cli.ts</files>
  <action>
Create `src/cli/commands/extract.ts`:

Export `extractCommand(args: string[])` async function:

1. Parse flags:
   - `--output-dir <dir>` (string, optional, default ".uidna")
   - `--help` (print extract-specific help and exit)

2. Load config via `loadFullConfig({})` to get outputDir (from config file or default).

3. Verify crawl data exists: Check for `.uidna/tokens/` directory (or whatever outputDir/tokens/ is). If no crawl data found, print "No crawl data found. Run `uidna crawl <url>` first." and exit(1).

4. Phase 1: Load raw token data from storage using `TokenStore` from `../../storage/index.js`:
   - Read aggregated tokens from the store
   - Use `withProgress` spinner: "Loading crawl data..."

5. Phase 2: Run normalization pipeline using `runNormalizationPipeline` (or the appropriate function from `../../normalization/normalize-pipeline.js`):
   - Use `withProgress` spinner: "Normalizing tokens..."
   - This produces NormalizationResult with deduplicated tokens in DTCG format

6. Phase 3: Run component detection and aggregation:
   - Use functions from `../../components/index.js`
   - Use `withProgress` spinner: "Detecting components..."

7. Phase 4: Run pattern detection and content analysis:
   - Use functions from `../../patterns/index.js` and `../../content/index.js`
   - Use `withProgress` spinner: "Analyzing patterns..."

8. Save results to outputDir (write normalized tokens, components, patterns to JSON files in .uidna/)

9. Print summary: token counts (normalized), component types found, patterns detected, output paths.

10. Extract-specific help:
    ```
    Usage: uidna extract [options]

    Run extraction pipeline on crawled data (normalize tokens, detect components, analyze patterns).

    Options:
      --output-dir <dir>  Output directory (default: .uidna)
    ```

NOTE: The extract command needs to read stored crawl data and run the Phase 2-4 processing pipeline. Read the TokenStore and existing storage modules to understand how to load persisted data. The extract command orchestrates: load stored tokens -> normalize -> detect components -> analyze content patterns -> save results.

Update `src/cli.ts` to wire the `extract` case similarly to crawl.
  </action>
  <verify>Run `npx tsc --noEmit` to confirm no type errors. Run `npx tsx src/cli.ts extract --help` to see extract-specific help. Run `npx tsx src/cli.ts extract` with no crawl data to see "No crawl data found" error.</verify>
  <done>Users can run `uidna extract` to process crawled data through the full normalization, component detection, and pattern analysis pipeline.</done>
</task>

</tasks>

<verification>
- `npx tsc --noEmit` passes
- `uidna crawl --help` shows crawl-specific help
- `uidna crawl` (no URL) exits with error about missing URL
- `uidna extract --help` shows extract-specific help
- `uidna extract` with no crawl data shows "run crawl first" message
- Both commands show progress spinners and summaries when data is available
</verification>

<success_criteria>
- crawl command accepts URL, depth, pages, domain options and runs the crawl pipeline with progress
- extract command loads crawl data and runs normalization + component + pattern detection
- Both commands provide clear error messages when preconditions not met
- Both integrated into CLI router
</success_criteria>

<output>
After completion, create `.planning/phases/06-cli-mcp-integration/06-02-SUMMARY.md`
</output>
