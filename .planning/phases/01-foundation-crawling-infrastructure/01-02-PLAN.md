---
phase: 01-foundation-crawling-infrastructure
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/crawler/robots-validator.ts
  - src/crawler/stealth-config.ts
  - src/crawler/wait-strategies.ts
  - src/crawler/playwright-crawler.ts
  - src/crawler/index.ts
autonomous: true

must_haves:
  truths:
    - "Crawler respects robots.txt and skips disallowed URLs"
    - "Crawler rate-limits requests to configurable maxRequestsPerMinute"
    - "Crawler detects and waits for CSS-in-JS style injection before extraction"
    - "Crawler uses stealth configuration to avoid bot detection"
    - "Crawler discovers and enqueues links within configured depth and domain constraints"
    - "CrawlerHandlers.onPageCrawled passes both PageData and the live Playwright Page for downstream extraction"
  artifacts:
    - path: "src/crawler/robots-validator.ts"
      provides: "robots.txt compliance checking"
      exports: ["RobotsValidator"]
    - path: "src/crawler/stealth-config.ts"
      provides: "Playwright stealth and anti-bot configuration"
      exports: ["createStealthBrowser"]
    - path: "src/crawler/wait-strategies.ts"
      provides: "Framework detection and hydration wait strategies"
      exports: ["waitForContentReady", "detectFramework", "detectCSSInJSLibrary"]
    - path: "src/crawler/playwright-crawler.ts"
      provides: "Main crawler orchestration using Crawlee PlaywrightCrawler"
      exports: ["createCrawler", "runCrawl", "CrawlerHandlers"]
  key_links:
    - from: "src/crawler/playwright-crawler.ts"
      to: "src/crawler/robots-validator.ts"
      via: "checks isAllowed() before processing each URL"
      pattern: "robotsValidator.*isAllowed"
    - from: "src/crawler/playwright-crawler.ts"
      to: "src/crawler/wait-strategies.ts"
      via: "calls waitForContentReady() in request handler"
      pattern: "waitForContentReady"
    - from: "src/crawler/playwright-crawler.ts"
      to: "src/crawler/stealth-config.ts"
      via: "uses stealth browser in launchContext"
      pattern: "createStealthBrowser"
    - from: "src/crawler/playwright-crawler.ts"
      to: "src/types/crawl-config.ts"
      via: "accepts CrawlConfig for all settings"
      pattern: "CrawlConfig"
---

<objective>
Build the complete Playwright-based web crawler with robots.txt compliance, rate limiting, anti-bot stealth, and dynamic content wait strategies.

Purpose: This is the core data acquisition layer. It must reliably navigate target sites, handle modern web frameworks (React, Vue, Angular with CSS-in-JS), respect crawl etiquette (robots.txt, rate limits), and avoid bot detection. All downstream token extraction depends on the crawler delivering fully-rendered pages.

Output: A working crawler that accepts a CrawlConfig, crawls seed URLs respecting all constraints, and returns PageData objects with fully-rendered content ready for token extraction. The CrawlerHandlers interface passes the live Playwright Page object to enable downstream extractors to run while the page is still open.
</objective>

<execution_context>
@C:/Users/Karl/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Karl/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@C:/Users/Karl/UIUX-Mirror/.planning/PROJECT.md
@C:/Users/Karl/UIUX-Mirror/.planning/ROADMAP.md
@C:/Users/Karl/UIUX-Mirror/.planning/phases/01-foundation-crawling-infrastructure/01-RESEARCH.md
@C:/Users/Karl/UIUX-Mirror/.planning/phases/01-foundation-crawling-infrastructure/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement robots.txt validator and stealth browser configuration</name>
  <files>src/crawler/robots-validator.ts, src/crawler/stealth-config.ts</files>
  <action>
Create the crawl compliance and stealth modules.

1. src/crawler/robots-validator.ts -- robots.txt compliance (CRAWL-02):
   - Create RobotsValidator class that:
     a. Fetches and caches robots.txt per domain (24h TTL using in-memory Map with timestamps)
     b. Uses `robots-parser` library to parse robots.txt content
     c. Exposes isAllowed(url: string, userAgent: string): Promise<boolean>
     d. Exposes getCrawlDelay(domain: string, userAgent: string): Promise<number | null>
     e. Handles missing robots.txt (404 = all allowed), network errors (configurable: strict mode = block, permissive = allow)
     f. Constructor takes strictMode: boolean (default true per PROJECT.md)
   - Cache implementation: Map<string, { parser: RobotsParser, fetchedAt: number }>
   - If robots.txt fetch fails with network error and strictMode is true, block all URLs for that domain
   - Log blocked URLs at debug level

2. src/crawler/stealth-config.ts -- Anti-bot stealth (CRAWL-05):
   - Create createStealthBrowser() function that:
     a. Uses playwright-extra with chromium
     b. Applies puppeteer-extra-plugin-stealth
     c. Returns configured chromium launcher
   - Create getRandomUserAgent(userAgents: string[]): string -- picks random UA from config list
   - Create getRandomViewport(viewports: Array<{width, height}>): {width, height} -- picks random viewport
   - Create addTimingJitter(minMs: number, maxMs: number): Promise<void> -- random delay using sleep + randomJitter from shared/utils

NOTE: The stealth plugin integration with playwright-extra may need adjustment based on actual API. Use `playwright-extra` package's chromium export and `.use()` method for the stealth plugin. If the stealth plugin import path has changed, check node_modules for the correct export. The research shows this pattern:
```
import { chromium } from 'playwright-extra';
import StealthPlugin from 'puppeteer-extra-plugin-stealth';
chromium.use(StealthPlugin());
```
Verify this works and adjust imports if needed.
  </action>
  <verify>
Write a quick test: `npx tsx -e "import { RobotsValidator } from './src/crawler/robots-validator.js'; const rv = new RobotsValidator(true); const allowed = await rv.isAllowed('https://example.com/', 'UIDNABot'); console.log('allowed:', allowed);"` -- should return true (example.com has no restrictive robots.txt).
Verify stealth module imports: `npx tsx -e "import { createStealthBrowser } from './src/crawler/stealth-config.js'; console.log('stealth OK');"` -- should not throw.
  </verify>
  <done>RobotsValidator fetches, caches, and enforces robots.txt with configurable strict mode. Stealth config provides anti-bot browser launcher with UA rotation and timing jitter.</done>
</task>

<task type="auto">
  <name>Task 2: Implement framework detection and dynamic content wait strategies</name>
  <files>src/crawler/wait-strategies.ts</files>
  <action>
Create the dynamic content handling module (CRAWL-04).

src/crawler/wait-strategies.ts:

1. detectFramework(page: Page): Promise<PageData['framework']>
   - Evaluate in page context to detect framework:
     - React: window.__NEXT_DATA__ or window.__REACT_DEVTOOLS_GLOBAL_HOOK__ or document.querySelector('[data-reactroot]')
     - Vue: window.__NUXT__ or window.$nuxt or window.__VUE__
     - Angular: window.ng or document.querySelector('[ng-version]')
     - Svelte: document.querySelector('[class*="svelte-"]')
     - Unknown: fallback

2. detectCSSInJSLibrary(page: Page): Promise<PageData['cssInJsLibrary']>
   - Check DOM for CSS-in-JS markers:
     - Emotion: style[data-emotion]
     - styled-components: style[data-styled] or elements with class matching /^sc-/
     - Stitches: style[data-stitches]
     - None: fallback

3. waitForFrameworkHydration(page: Page, framework: string): Promise<void>
   - React/Next.js: wait for __NEXT_DATA__?.hydrated !== false (5s timeout, catch and continue)
   - Angular: wait for [ng-version] attached + 500ms buffer
   - Vue/Nuxt: wait for [data-v-app] or __NUXT__.isHydrating === false (5s timeout)
   - Unknown: skip (no framework-specific wait)

4. waitForStyleInjection(page: Page): Promise<void>
   - Wait for style[data-emotion] OR style[data-styled] OR style[data-stitches] OR document.styleSheets.length > 0
   - 3 second timeout, catch and continue (site may not use CSS-in-JS)

5. waitForContentReady(page: Page): Promise<{ framework: string; cssInJsLibrary: string }>
   - Orchestrates the full wait sequence:
     a. page.waitForLoadState('networkidle') with 10s timeout
     b. detectFramework(page)
     c. waitForFrameworkHydration(page, framework)
     d. detectCSSInJSLibrary(page)
     e. waitForStyleInjection(page)
     f. Additional 200ms buffer for any final style calculations
     g. Return detected framework and CSS-in-JS library
   - Log detected framework and CSS-in-JS library at debug level
   - All waits use try/catch with timeout -- never block the crawl indefinitely

All page.evaluate() calls should be wrapped in try/catch. If a wait fails, log a warning and continue -- partial extraction is better than no extraction.
  </action>
  <verify>Run `npx tsc --noEmit` -- should compile with zero errors. The wait strategies can only be fully tested against real sites (covered in integration plan), but type checking ensures correctness of Playwright API usage.</verify>
  <done>Framework detection identifies React/Vue/Angular/Svelte. CSS-in-JS detection identifies Emotion/styled-components/Stitches. Wait strategies handle hydration and style injection with safe timeouts.</done>
</task>

<task type="auto">
  <name>Task 3: Build the main PlaywrightCrawler with Crawlee integration and extractor-ready handler interface</name>
  <files>src/crawler/playwright-crawler.ts, src/crawler/index.ts</files>
  <action>
Create the main crawler that ties everything together using Crawlee's PlaywrightCrawler (CRAWL-01, CRAWL-03, CRAWL-05).

1. src/crawler/playwright-crawler.ts:

Create a createCrawler(config: CrawlConfig, handlers: CrawlerHandlers) function that:

a. Sets up Crawlee PlaywrightCrawler with:
   - launchContext using createStealthBrowser() from stealth-config
   - maxConcurrency from config.maxConcurrency
   - maxRequestsPerMinute from config.maxRequestsPerMinute
   - maxRequestRetries: 3
   - requestHandlerTimeoutSecs: 60

b. CrawlerHandlers interface -- CRITICAL: designed for extractor integration:
   ```typescript
   export interface CrawlerHandlers {
     /** Called after page is fully loaded and ready. The live Playwright Page is passed
      *  so downstream extractors can run computed style queries while the page is still open.
      *  Extractors MUST complete before this callback returns -- page may be recycled after. */
     onPageCrawled(pageData: PageData, page: Page): Promise<void>;
     onPageFailed(url: string, error: Error): void;
     onPageSkipped(url: string, reason: string): void;
   }
   ```
   The `page: Page` parameter (from 'playwright') is essential. Downstream plans (01-04, 01-05, 01-06) will use it to run extractAllTokens(page, url) inside the callback. The page must still be live (not closed/recycled) when onPageCrawled is called.

c. Request handler implementation:
   1. Check robots.txt via RobotsValidator.isAllowed(). If blocked, call onPageSkipped and return.
   2. Apply timing jitter (addTimingJitter from stealth-config)
   3. Set random user agent and viewport on page context
   4. Navigate to URL (page.goto with waitUntil: 'domcontentloaded')
   5. Call waitForContentReady() for framework detection and style injection wait
   6. Capture full-page screenshot to config.outputDir/screenshots/{sanitizedUrl}.png
   7. Build PageData object with url, finalUrl (page.url()), statusCode, title, timestamp, framework, cssInJsLibrary, htmlContent (page.content()), screenshotPath
   8. Call `await handlers.onPageCrawled(pageData, page)` -- AWAIT the handler so extractors finish before Crawlee recycles the page
   9. Enqueue discovered links using Crawlee's enqueueLinks with:
      - strategy: 'same-domain' (or filter by domainAllowlist if provided)
      - Respect maxDepth from config
      - Filter out non-HTML resources (.jpg, .png, .css, .js, .pdf, .zip)
      - transformRequestFunction to apply domain allowlist filtering

d. Failed request handler: call onPageFailed with error details

Create runCrawl(config: CrawlConfig, handlers: CrawlerHandlers): Promise<CrawlResult> that:
   1. Creates RobotsValidator with config.respectRobotsTxt
   2. Creates the crawler via createCrawler
   3. Ensures output directory exists (fs-extra ensureDir)
   4. Runs crawler.run(config.seedUrls)
   5. Returns CrawlResult with timing, page counts, and collected PageData

IMPORTANT architectural decisions:
- Do NOT create a new browser context per page (memory leak per research). Let Crawlee manage the browser pool.
- The onPageCrawled handler receives the live Page -- this is the hook where extractors plug in (Plans 04/05/06). The page MUST still be navigable when the callback fires.
- Store PageData to disk incrementally via the handler, don't accumulate in memory.
- Use createLogger('crawler') for all logging.

2. src/crawler/index.ts -- Barrel export:
   - Export { createCrawler, runCrawl } from './playwright-crawler.js'
   - Export type { CrawlerHandlers } from './playwright-crawler.js'
   - Export { RobotsValidator } from './robots-validator.js'
   - Export { createStealthBrowser, getRandomUserAgent, getRandomViewport, addTimingJitter } from './stealth-config.js'
   - Export { waitForContentReady, detectFramework, detectCSSInJSLibrary } from './wait-strategies.js'
  </action>
  <verify>
Run `npx tsc --noEmit` -- should compile with zero errors.
Run a smoke test: `npx tsx -e "import { runCrawl } from './src/crawler/index.js'; import { loadConfig } from './src/shared/index.js'; const config = loadConfig({ seedUrls: ['https://example.com'], maxPages: 1, maxDepth: 0 }); const result = await runCrawl(config, { onPageCrawled: async (pageData, page) => { console.log('Crawled:', pageData.url, 'Status:', pageData.statusCode); console.log('Page still live:', typeof page.url === 'function'); }, onPageFailed: (u, e) => console.error('Failed:', u, e.message), onPageSkipped: (u, r) => console.log('Skipped:', u, r) }); console.log('Pages processed:', result.pagesProcessed);"` -- should crawl example.com, pass both pageData and live page to handler, and print status.
  </verify>
  <done>Crawler accepts CrawlConfig, crawls seed URLs with Crawlee PlaywrightCrawler, respects robots.txt, enforces rate limits, uses stealth browser, waits for dynamic content, and delivers PageData + live Playwright Page through handler callbacks. CrawlerHandlers.onPageCrawled(pageData, page) signature enables downstream extractor integration without architectural rework. Single-page crawl of example.com succeeds.</done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with zero errors
2. Crawl of https://example.com (1 page, depth 0) succeeds and returns PageData
3. onPageCrawled handler receives both PageData and a live Playwright Page object
4. RobotsValidator correctly blocks URLs disallowed by robots.txt
5. Framework detection returns 'unknown' for example.com (no framework)
6. Screenshot is saved to .uidna/screenshots/
7. No memory leaks: single page crawl doesn't leave hanging processes
</verification>

<success_criteria>
- CRAWL-01: Crawler processes seed URLs with configurable depth (maxDepth) and page limit (maxPages)
- CRAWL-02: RobotsValidator checks robots.txt before every request, respects strict mode
- CRAWL-03: Crawlee enforces maxRequestsPerMinute and maxConcurrency from config
- CRAWL-04: waitForContentReady detects frameworks and waits for CSS-in-JS injection
- CRAWL-05: Stealth browser with UA rotation, viewport randomization, timing jitter
- CrawlerHandlers interface passes live Page to onPageCrawled for extractor integration
- PageData objects contain all fields needed for downstream token extraction
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-crawling-infrastructure/01-02-SUMMARY.md`
</output>
