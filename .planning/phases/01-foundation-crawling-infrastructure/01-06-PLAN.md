---
phase: 01-foundation-crawling-infrastructure
plan: 06
type: execute
wave: 5
depends_on: ["01-02", "01-03", "01-04", "01-05"]
files_modified:
  - src/storage/token-store.ts
  - src/storage/diff-tracker.ts
  - src/storage/index.ts
  - src/orchestrator.ts
  - src/cli.ts
  - src/index.ts
autonomous: true

must_haves:
  truths:
    - "Complete crawl+extract pipeline runs end-to-end from seed URL to stored tokens with evidence"
    - "Tokens and evidence are persisted to .uidna/ directory as JSON files"
    - "Re-crawl detects added, removed, changed, and unchanged pages via token hash comparison"
    - "Diff report clearly shows what changed since last crawl"
    - "User can run the tool from CLI via npx uidna or the bin entry point"
  artifacts:
    - path: "src/orchestrator.ts"
      provides: "End-to-end pipeline orchestration: crawl -> extract -> store"
      exports: ["runPipeline"]
    - path: "src/storage/token-store.ts"
      provides: "Token persistence to JSON files"
      exports: ["TokenStore"]
    - path: "src/storage/diff-tracker.ts"
      provides: "Crawl snapshot comparison and diff detection"
      exports: ["DiffTracker"]
    - path: "src/cli.ts"
      provides: "CLI entry point for running the pipeline"
      min_lines: 30
  key_links:
    - from: "src/orchestrator.ts"
      to: "src/crawler/index.ts"
      via: "calls runCrawl with extraction handlers"
      pattern: "runCrawl"
    - from: "src/orchestrator.ts"
      to: "src/extractors/index.ts"
      via: "calls extractAllTokens in onPageCrawled handler"
      pattern: "extractAllTokens"
    - from: "src/orchestrator.ts"
      to: "src/evidence/index.ts"
      via: "stores evidence for each extracted token"
      pattern: "EvidenceStore"
    - from: "src/orchestrator.ts"
      to: "src/storage/token-store.ts"
      via: "persists extracted tokens per page"
      pattern: "TokenStore"
    - from: "src/storage/diff-tracker.ts"
      to: "src/storage/token-store.ts"
      via: "compares current token hashes against previous snapshot"
      pattern: "TokenStore|CrawlSnapshot"
    - from: "src/cli.ts"
      to: "src/orchestrator.ts"
      via: "parses CLI args and calls runPipeline"
      pattern: "runPipeline"
---

<objective>
Wire together the crawler, extractors, evidence store, and storage into a complete pipeline with diff tracking for re-crawls, plus a CLI entry point so users can actually run the tool.

Purpose: This is the integration plan that makes everything work end-to-end. A user should be able to run the pipeline with a seed URL and get a .uidna/ directory containing all extracted tokens with full evidence. Running it again should produce a diff showing what changed. This covers the remaining requirements: CRAWL-06 (re-crawl diff) and completing the CRAWL-01 integration. The CLI entry point fulfills the bin.uidna definition from package.json.

Output: A runPipeline function that orchestrates crawl -> extract -> store -> (optional) diff, a CLI that exposes it to users, and a complete .uidna/ output directory.
</objective>

<execution_context>
@C:/Users/Karl/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Karl/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@C:/Users/Karl/UIUX-Mirror/.planning/PROJECT.md
@C:/Users/Karl/UIUX-Mirror/.planning/ROADMAP.md
@C:/Users/Karl/UIUX-Mirror/.planning/phases/01-foundation-crawling-infrastructure/01-RESEARCH.md
@C:/Users/Karl/UIUX-Mirror/.planning/phases/01-foundation-crawling-infrastructure/01-01-SUMMARY.md
@C:/Users/Karl/UIUX-Mirror/.planning/phases/01-foundation-crawling-infrastructure/01-02-SUMMARY.md
@C:/Users/Karl/UIUX-Mirror/.planning/phases/01-foundation-crawling-infrastructure/01-03-SUMMARY.md
@C:/Users/Karl/UIUX-Mirror/.planning/phases/01-foundation-crawling-infrastructure/01-04-SUMMARY.md
@C:/Users/Karl/UIUX-Mirror/.planning/phases/01-foundation-crawling-infrastructure/01-05-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement TokenStore and DiffTracker for persistence and change detection</name>
  <files>src/storage/token-store.ts, src/storage/diff-tracker.ts, src/storage/index.ts</files>
  <action>
Create the storage layer for persisting tokens and detecting changes across crawls.

1. src/storage/token-store.ts -- Token persistence:

TokenStore class with constructor(outputDir: string):
   - outputDir: base directory for token storage (e.g., '.uidna/tokens')
   - Constructor ensures directory exists via fs-extra ensureDirSync

Methods:

a. async savePageTokens(pageUrl: string, tokens: PageTokens): Promise<void>
   - Serialize tokens to JSON
   - Write to {outputDir}/page-{urlHash}.json where urlHash = sanitizeFilename(pageUrl)
   - Use fs-extra writeJson with { spaces: 2 } for readable output
   - Log at debug level

b. async loadPageTokens(pageUrl: string): Promise<PageTokens | null>
   - Read from {outputDir}/page-{urlHash}.json
   - Return null if file doesn't exist
   - Parse and return PageTokens

c. async saveAggregatedTokens(allTokens: Record<string, PageTokens>): Promise<void>
   - Merge tokens across all pages into aggregated files:
     - {outputDir}/all-colors.json (all ColorToken[] across pages, merged evidence)
     - {outputDir}/all-typography.json
     - {outputDir}/all-spacing.json
     - etc. for each token type
   - Also write {outputDir}/summary.json with token counts per category

d. async getAllPageUrls(): Promise<string[]>
   - Scan outputDir for page-*.json files
   - Return list of page URLs (reverse the sanitization from filename)

2. src/storage/diff-tracker.ts -- Change detection (CRAWL-06):

DiffTracker class with constructor(outputDir: string):
   - outputDir: base directory for snapshots (e.g., '.uidna/snapshots')
   - Constructor ensures directory exists

Methods:

a. async saveSnapshot(config: CrawlConfig, pageTokenHashes: Record<string, string>): Promise<CrawlSnapshot>
   - Create CrawlSnapshot: { timestamp, configHash (hash of config), tokenHashes: pageTokenHashes, totalPages }
   - Write to {outputDir}/snapshot-{timestamp}.json
   - Also write {outputDir}/latest-snapshot.json (overwrite with current)
   - Return the snapshot

b. async loadLatestSnapshot(): Promise<CrawlSnapshot | null>
   - Read {outputDir}/latest-snapshot.json
   - Return null if doesn't exist (first crawl)

c. async computeDiff(previous: CrawlSnapshot, current: CrawlSnapshot): Promise<DiffResult>
   - Compare tokenHashes maps:
     - added: URLs in current but not in previous
     - removed: URLs in previous but not in current
     - changed: URLs in both but with different token hashes
     - unchanged: URLs in both with same token hashes
   - Return DiffResult object

d. async generateDiffReport(diff: DiffResult): Promise<string>
   - Generate human-readable diff report (plain text):
     ```
     Crawl Diff Report
     =================
     Previous: {timestamp}
     Current: {timestamp}

     Summary:
     - Pages added: {N}
     - Pages removed: {N}
     - Pages changed: {N}
     - Pages unchanged: {N}

     Added pages:
     - https://example.com/new-page

     Changed pages:
     - https://example.com/about (token hash changed)

     Removed pages:
     - https://example.com/old-page
     ```
   - Write report to {outputDir}/diff-report-{timestamp}.txt
   - Return the report string

3. src/storage/index.ts -- Barrel export:
   - Export { TokenStore } from './token-store.js'
   - Export { DiffTracker } from './diff-tracker.js'
  </action>
  <verify>
Run `npx tsc --noEmit` -- should compile with zero errors.
Run: `npx tsx -e "
import { TokenStore } from './src/storage/index.js';
const store = new TokenStore('.uidna/test-tokens');
await store.savePageTokens('https://example.com', { colors: [], typography: [], spacing: [], customProperties: [], radii: [], shadows: [], zIndexes: [], motion: [], icons: [], imagery: [] });
const loaded = await store.loadPageTokens('https://example.com');
console.log('Loaded:', loaded !== null);
"` -- should save and reload tokens.
Clean up: `rm -rf .uidna/test-tokens`
  </verify>
  <done>TokenStore persists per-page tokens and aggregated summaries to JSON. DiffTracker saves crawl snapshots, loads previous snapshots, computes diffs (added/removed/changed/unchanged), and generates human-readable diff reports.</done>
</task>

<task type="auto">
  <name>Task 2: Build the end-to-end pipeline orchestrator</name>
  <files>src/orchestrator.ts, src/index.ts</files>
  <action>
Create the main pipeline orchestrator that wires crawler -> extractors -> evidence -> storage. This plan USES the CrawlerHandlers interface from Plan 01-02 which already defines onPageCrawled(pageData: PageData, page: Page) -- no architectural modifications needed.

1. src/orchestrator.ts:

Export interface PipelineOptions:
   - config: CrawlConfig
   - onProgress?: (status: { pagesProcessed: number; totalEnqueued: number; currentUrl: string }) => void

Export async function runPipeline(options: PipelineOptions): Promise<PipelineResult>

PipelineResult interface:
   - crawlResult: CrawlResult
   - tokenSummary: { colors: number; typography: number; spacing: number; customProperties: number; radii: number; shadows: number; zIndexes: number; motion: number; icons: number; imagery: number }
   - evidenceCount: number
   - diffResult?: DiffResult
   - outputDir: string

Implementation:

a. Initialize services:
   - const tokenStore = new TokenStore(path.join(config.outputDir, 'tokens'))
   - const evidenceStore = new EvidenceStore(path.join(config.outputDir, 'evidence'))
   - const screenshotManager = new ScreenshotManager(path.join(config.outputDir, 'evidence', 'screenshots'))
   - const diffTracker = new DiffTracker(path.join(config.outputDir, 'snapshots'))
   - const logger = createLogger('orchestrator')

b. Load previous snapshot for diff detection:
   - const previousSnapshot = await diffTracker.loadLatestSnapshot()

c. Define crawler handlers (using CrawlerHandlers from Plan 01-02):
   - pageTokenHashes: Record<string, string> = {} (for snapshot)
   - pagesProcessed = 0

   onPageCrawled handler -- receives (pageData: PageData, page: Page):
   The live Playwright Page object is available directly from the CrawlerHandlers interface defined in Plan 01-02. Use it to run extractors while the page is still open.

   1. Extract tokens: const tokens = await extractAllTokens(page, pageData.url)
      The extractors need the live Playwright Page for computed style queries. This is provided directly by the onPageCrawled(pageData, page) callback signature.

   2. Store tokens per page: await tokenStore.savePageTokens(pageData.url, tokens)
   3. Store evidence for a SAMPLE of tokens (not every element -- would be too many):
      For each token type, pick up to 10 representative tokens and store their evidence:
      ```
      for (const token of tokens.colors.slice(0, 10)) {
        for (const ev of token.evidence) {
          await evidenceStore.addEvidence({
            pageUrl: ev.pageUrl,
            selector: ev.selector,
            computedStyles: ev.computedStyles,
            screenshotPath: ev.screenshotPath,
            boundingBox: ev.boundingBox
          });
        }
      }
      ```
      Also capture element screenshots for top-evidence tokens using screenshotManager.captureElement
   4. Compute token hash for this page: pageTokenHashes[pageData.url] = hashTokens(tokens)
   5. Flush evidence store periodically (every page for now, optimize later if needed)
   6. Call onProgress callback if provided
   7. pagesProcessed++

d. Run the crawl: const crawlResult = await runCrawl(config, handlers)

e. After crawl completes:
   1. Save aggregated tokens: await tokenStore.saveAggregatedTokens(allPageTokens)
   2. Flush evidence: await evidenceStore.flush()
   3. Save snapshot: await diffTracker.saveSnapshot(config, pageTokenHashes)
   4. If previousSnapshot exists, compute diff:
      const diff = await diffTracker.computeDiff(previousSnapshot, currentSnapshot)
      const report = await diffTracker.generateDiffReport(diff)
      logger.info(`Diff report:\n${report}`)

f. Return PipelineResult with all results

2. Update src/index.ts:
   - Export { runPipeline } from './orchestrator.js'
   - Export all types, shared, crawler, extractors, evidence, storage modules
   - This becomes the library's main entry point
  </action>
  <verify>
Run `npx tsc --noEmit` -- should compile with zero errors.
Run end-to-end test against example.com:
```
npx tsx -e "
import { runPipeline } from './src/index.js';
import { loadConfig } from './src/shared/index.js';
const config = loadConfig({ seedUrls: ['https://example.com'], maxPages: 1, maxDepth: 0 });
const result = await runPipeline({ config, onProgress: (s) => console.log('Progress:', s.currentUrl) });
console.log('Pages:', result.crawlResult.pagesProcessed);
console.log('Colors:', result.tokenSummary.colors);
console.log('Typography:', result.tokenSummary.typography);
console.log('Evidence:', result.evidenceCount);
console.log('Output:', result.outputDir);
"
```
Should crawl example.com, extract tokens, save to .uidna/, and print summary.
Verify output files exist: `ls .uidna/tokens/ .uidna/evidence/ .uidna/snapshots/`
  </verify>
  <done>Pipeline orchestrates crawl->extract->store->diff in a single runPipeline call. Uses CrawlerHandlers.onPageCrawled(pageData, page) interface from Plan 01-02 to access live Page for extraction -- no architectural modifications needed. Tokens are persisted per-page and aggregated. Evidence is stored with screenshots. Snapshots enable re-crawl diff detection. End-to-end test against example.com produces valid output.</done>
</task>

<task type="auto">
  <name>Task 3: Implement CLI entry point for user-facing pipeline execution</name>
  <files>src/cli.ts</files>
  <action>
Create the CLI entry point that allows users to run the pipeline from the command line. This fulfills the bin.uidna definition in package.json and satisfies CRAWL-06's requirement that users can re-crawl and see diffs.

src/cli.ts:

1. Add shebang line: #!/usr/bin/env node

2. Parse command line arguments (use process.argv directly, no external CLI framework needed for v1):
   - First positional arg: seed URL (required)
   - --max-pages N (default: 100)
   - --max-depth N (default: 3)
   - --output-dir DIR (default: '.uidna')
   - --concurrency N (default: 5)
   - --no-robots (disable robots.txt checking, default: respect robots.txt)
   - --help (print usage and exit)

3. Usage message:
   ```
   Usage: uidna <seed-url> [options]

   Options:
     --max-pages N      Maximum pages to crawl (default: 100)
     --max-depth N      Maximum link depth (default: 3)
     --output-dir DIR   Output directory (default: .uidna)
     --concurrency N    Max concurrent requests (default: 5)
     --no-robots        Disable robots.txt checking
     --help             Show this help message

   Examples:
     uidna https://example.com
     uidna https://tailwindcss.com --max-pages 20 --max-depth 2
     uidna https://mysite.com --output-dir ./analysis
   ```

4. Build CrawlConfig from parsed args using loadConfig with overrides:
   ```typescript
   const config = loadConfig({
     seedUrls: [seedUrl],
     maxPages: parsedArgs.maxPages,
     maxDepth: parsedArgs.maxDepth,
     outputDir: parsedArgs.outputDir,
     maxConcurrency: parsedArgs.concurrency,
     respectRobotsTxt: !parsedArgs.noRobots,
   });
   ```

5. Run pipeline with progress output:
   ```typescript
   const result = await runPipeline({
     config,
     onProgress: (status) => {
       console.log(`[${status.pagesProcessed}] Crawling: ${status.currentUrl}`);
     },
   });
   ```

6. Print summary after completion:
   ```
   Crawl Complete
   ==============
   Pages crawled: {N}
   Pages failed: {N}
   Pages skipped (robots.txt): {N}

   Tokens extracted:
     Colors: {N}
     Typography: {N}
     Spacing: {N}
     Custom Properties: {N}
     Border Radii: {N}
     Shadows: {N}
     Z-Index: {N}
     Motion: {N}
     Icons: {N}
     Imagery: {N}

   Evidence entries: {N}
   Output directory: {outputDir}
   ```

7. If diff exists (re-crawl), also print the diff summary.

8. Handle errors: wrap in try/catch, print error message with stack trace if LOG_LEVEL=debug, exit with code 1.

9. Use process.exit(0) on success, process.exit(1) on error.
  </action>
  <verify>
Run `npx tsc --noEmit` -- should compile with zero errors.
Run CLI directly: `npx tsx src/cli.ts https://example.com --max-pages 1 --max-depth 0` -- should crawl example.com and print summary.
Run CLI with help: `npx tsx src/cli.ts --help` -- should print usage message.
Run CLI without args: `npx tsx src/cli.ts` -- should print usage and exit with code 1.
  </verify>
  <done>CLI entry point parses seed URL and options from command line, calls runPipeline, and prints human-readable summary. Users can run `npx uidna https://example.com` or `npx tsx src/cli.ts` to execute the full crawl+extract pipeline. Re-crawl diff is printed automatically when a previous snapshot exists.</done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with zero errors
2. `runPipeline` with example.com produces:
   - .uidna/tokens/ with page-level and aggregated JSON files
   - .uidna/evidence/ with evidence-index.json
   - .uidna/snapshots/ with latest-snapshot.json
3. Running pipeline twice produces a diff report (second run shows "unchanged" for same content)
4. Token summary shows non-zero counts for at least colors and typography
5. CLI runs successfully with `npx tsx src/cli.ts https://example.com --max-pages 1`
6. CLI prints help with `--help` flag
7. No memory leaks or hanging processes after pipeline completes
</verification>

<success_criteria>
- CRAWL-01 (complete): Full pipeline from seed URL to stored results with configurable depth and page limits
- CRAWL-06: Re-crawl produces diff report showing added/removed/changed/unchanged pages
- CRAWL-06 (user-facing): CLI entry point allows users to run the tool and see results
- NORM-03 (integration): Evidence is stored per observation with all required fields
- All 8 TOKEN requirements: Extractors run on every crawled page and tokens are persisted
- Pipeline handles errors gracefully (failed pages logged, not fatal)
- Output directory (.uidna/) contains human-inspectable JSON files
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-crawling-infrastructure/01-06-SUMMARY.md`
</output>
